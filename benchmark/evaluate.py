#!/usr/bin/env python3
"""
Unified Evaluation Script for Path Coverage Benchmark

Evaluates test cases generated by different approaches:
- Zero-shot baseline
- AutoExe
- Agentic-Z3

Metrics:
- Syntax correctness
- Execution correctness  
- Path exact match accuracy
- Path similarity score (LCS-based)
"""

import os
import sys
import json
import signal
import re
import shutil
import time
from pathlib import Path
from argparse import ArgumentParser
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import dataclass, field
from tqdm import tqdm

# Add benchmark directory to path
benchmark_dir = str(Path(__file__).parent)
if benchmark_dir not in sys.path:
    sys.path.insert(0, benchmark_dir)

# Import config
import importlib.util
config_path = Path(__file__).parent / "config.py"
spec = importlib.util.spec_from_file_location("benchmark_config", config_path)
benchmark_config = importlib.util.module_from_spec(spec)
spec.loader.exec_module(benchmark_config)

LEETCODE_INSTRUMENTED = benchmark_config.LEETCODE_INSTRUMENTED
TARGET_PATHS_DATA = benchmark_config.TARGET_PATHS_DATA
RESULTS_DIR = benchmark_config.RESULTS_DIR
EXECUTION_TIMEOUT = benchmark_config.EXECUTION_TIMEOUT


def read_jsonl(path: Path) -> List[Dict[str, Any]]:
    """Read a JSONL file."""
    data = []
    with open(path, 'r') as f:
        for line in f:
            data.append(json.loads(line))
    return data


class TimeoutHandler:
    """Context manager for execution timeout using signals."""
    
    def __init__(self, timeout: int, error_message: str = None):
        self.timeout = timeout
        self.error_message = error_message or f"Execution timed out after {timeout}s"
    
    def __enter__(self):
        if hasattr(signal, 'SIGALRM'):  # Unix only
            signal.signal(signal.SIGALRM, self.raise_timeout)
            signal.alarm(self.timeout)
        return self
    
    def __exit__(self, type, value, traceback):
        if hasattr(signal, 'SIGALRM'):
            signal.alarm(0)
    
    def raise_timeout(self, *args):
        raise TimeoutError(self.error_message)


def execute_test(test_code: str, timeout: int = EXECUTION_TIMEOUT) -> Tuple[bool, Optional[str]]:
    """
    Try to execute test code.
    
    Returns:
        Tuple of (success, error_message)
    """
    try:
        with TimeoutHandler(timeout):
            exec(test_code, globals())
            return True, None
    except AssertionError:
        # AssertionError is considered executable (test assertion failure)
        return True, None
    except TimeoutError:
        return False, "timeout"
    except Exception as e:
        return False, f"{type(e).__name__}: {str(e)[:100]}"


def match_path(generated_path: List[str], ref_path: List[str]) -> float:
    """
    Compute path similarity based on longest common subsequence.
    
    Returns:
        Similarity score: len(lcs(generated_path, ref_path)) / len(ref_path)
    """
    if not ref_path:
        return 1.0 if not generated_path else 0.0
    
    ref_len = len(ref_path)
    gen_len = len(generated_path)
    
    # DP table for LCS
    dp = [[0] * (ref_len + 1) for _ in range(gen_len + 1)]
    
    for i in range(1, gen_len + 1):
        for j in range(1, ref_len + 1):
            if generated_path[i-1] == ref_path[j-1]:
                dp[i][j] = dp[i-1][j-1] + 1
            else:
                dp[i][j] = max(dp[i-1][j], dp[i][j-1])
    
    lcs_length = dp[gen_len][ref_len]
    return lcs_length / ref_len


def extract_test_function(test_code: str) -> Optional[str]:
    """
    Extract test function from generated code.
    
    The generated code might include markdown formatting or extra text.
    """
    if not test_code:
        return None
    
    # Try to extract code from markdown code block
    code_block_match = re.search(r'```(?:python)?\s*(.*?)```', test_code, re.DOTALL)
    if code_block_match:
        test_code = code_block_match.group(1)
    
    # Look for test function definition
    func_match = re.search(r'(def test_\w+\(.*?\):.*?)(?=\ndef |\Z)', test_code, re.DOTALL)
    if func_match:
        return func_match.group(1).strip()
    
    # Return original if looks like a function
    if 'def test_' in test_code:
        return test_code.strip()
    
    return None


@dataclass
class EvaluationResult:
    """Results for a single approach."""
    approach: str
    total_tasks: int = 0
    total_paths: int = 0
    syntax_correct: int = 0
    execution_correct: int = 0
    path_exact_match: int = 0
    path_similarity_sum: float = 0.0
    errors: List[Dict[str, Any]] = field(default_factory=list)
    
    @property
    def syntax_rate(self) -> float:
        return self.syntax_correct / self.total_paths if self.total_paths > 0 else 0.0
    
    @property
    def execution_rate(self) -> float:
        return self.execution_correct / self.total_paths if self.total_paths > 0 else 0.0
    
    @property
    def exact_match_rate(self) -> float:
        return self.path_exact_match / self.total_paths if self.total_paths > 0 else 0.0
    
    @property
    def avg_similarity(self) -> float:
        return self.path_similarity_sum / self.total_paths if self.total_paths > 0 else 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "approach": self.approach,
            "total_tasks": self.total_tasks,
            "total_paths": self.total_paths,
            "syntax_correctness": f"{self.syntax_rate:.2%}",
            "execution_correctness": f"{self.execution_rate:.2%}",
            "path_exact_match": f"{self.exact_match_rate:.2%}",
            "path_similarity": f"{self.avg_similarity:.4f}",
            "syntax_correct_count": self.syntax_correct,
            "execution_correct_count": self.execution_correct,
            "path_exact_match_count": self.path_exact_match,
        }


class Evaluator:
    """
    Evaluator for path coverage benchmark.
    """
    
    def __init__(
        self,
        instrumented_data_path: Path = LEETCODE_INSTRUMENTED,
        paths_data_path: Path = TARGET_PATHS_DATA
    ):
        """Initialize evaluator with reference data."""
        print(f"Loading instrumented code from {instrumented_data_path}")
        self.instrumented_data = read_jsonl(instrumented_data_path)
        
        print(f"Loading target paths from {paths_data_path}")
        self.paths_data = read_jsonl(paths_data_path)
        
        # Build index by task_num
        self.task_index = {}
        for idx, (task, paths) in enumerate(zip(self.instrumented_data, self.paths_data)):
            task_num = task.get('task_num')
            if task_num is not None:
                self.task_index[task_num] = {
                    'index': idx,
                    'task': task,
                    'paths': paths
                }
    
    def evaluate_predictions(
        self,
        predictions_path: Path,
        approach_name: str = "unknown"
    ) -> EvaluationResult:
        """
        Evaluate predictions from a single approach.
        
        Args:
            predictions_path: Path to JSONL file with predictions
            approach_name: Name of the approach for reporting
            
        Returns:
            EvaluationResult with metrics
        """
        print(f"\nEvaluating {approach_name} from {predictions_path}")
        
        predictions = read_jsonl(predictions_path)
        result = EvaluationResult(approach=approach_name)
        result.total_tasks = len(predictions)
        
        # Create temp directory for execution
        temp_base = Path(f"tmp_eval_{approach_name}")
        temp_base.mkdir(exist_ok=True)
        
        try:
            for pred in tqdm(predictions, desc=f"Evaluating {approach_name}"):
                task_num = pred['task_num']
                func_name = pred['func_name']
                tests = pred.get('tests', [])
                
                # Get reference data
                if task_num not in self.task_index:
                    print(f"Warning: Task {task_num} not in reference data")
                    continue
                
                ref_data = self.task_index[task_num]
                ref_task = ref_data['task']
                ref_paths = ref_data['paths']
                
                # Get instrumented code and target paths
                instrumented_code = ref_task.get('python_solution_instrumented', '')
                sampled_paths = ref_paths.get('sampled_paths', [])
                task_title = ref_task.get('task_title', '')
                difficulty = pred.get('difficulty', 0)
                
                # Create task temp dir
                task_dir = temp_base / f"task_{task_num}_{difficulty}"
                task_dir.mkdir(exist_ok=True)
                
                # Write instrumented code
                (task_dir / "under_test.py").write_text(instrumented_code)
                log_dir = task_dir / "test_logs"
                log_dir.mkdir(exist_ok=True)
                
                # Evaluate each test for each path
                for path_idx, test_code in enumerate(tests):
                    result.total_paths += 1
                    
                    # Clear log file
                    log_file = log_dir / f"{task_title}.log"
                    log_file.write_text('')
                    
                    # Extract test function
                    clean_test = extract_test_function(test_code)
                    if clean_test is None:
                        result.errors.append({
                            'task': task_num,
                            'path': path_idx,
                            'error': 'no_test_function'
                        })
                        continue
                    
                    # Check syntax
                    try:
                        compile(clean_test, '<string>', 'exec')
                        result.syntax_correct += 1
                    except SyntaxError:
                        result.errors.append({
                            'task': task_num,
                            'path': path_idx,
                            'error': 'syntax_error'
                        })
                        continue
                    
                    # Build executable test code
                    test_import = f"from under_test import Solution\n"
                    full_test = test_import + clean_test + f"\ntest_{func_name}()"
                    
                    # Change to task directory for execution
                    original_cwd = os.getcwd()
                    os.chdir(task_dir)
                    
                    try:
                        # Execute test
                        success, error = execute_test(full_test)
                        
                        if success:
                            # Check if function was actually called
                            if f"solution.{func_name}" in full_test or f"Solution().{func_name}" in clean_test:
                                result.execution_correct += 1
                                
                                # Check path coverage
                                if path_idx < len(sampled_paths):
                                    ref_path = sampled_paths[path_idx]
                                    
                                    # Read executed path from log
                                    if log_file.exists():
                                        executed_lines = log_file.read_text().strip().split('\n')
                                        executed_path = [l for l in executed_lines if l]
                                        
                                        # Calculate similarity
                                        similarity = match_path(executed_path, ref_path)
                                        result.path_similarity_sum += similarity
                                        
                                        if similarity == 1.0:
                                            result.path_exact_match += 1
                                    else:
                                        # No log = couldn't trace path
                                        pass
                            else:
                                result.errors.append({
                                    'task': task_num,
                                    'path': path_idx,
                                    'error': 'function_not_called'
                                })
                        else:
                            result.errors.append({
                                'task': task_num,
                                'path': path_idx,
                                'error': error
                            })
                    finally:
                        os.chdir(original_cwd)
        
        finally:
            # Cleanup temp directory
            if temp_base.exists():
                shutil.rmtree(temp_base, ignore_errors=True)
        
        return result
    
    def evaluate_all(
        self,
        approaches: Dict[str, Path]
    ) -> Dict[str, EvaluationResult]:
        """
        Evaluate multiple approaches.
        
        Args:
            approaches: Dict mapping approach name to predictions file path
            
        Returns:
            Dict mapping approach name to EvaluationResult
        """
        results = {}
        
        for approach_name, predictions_path in approaches.items():
            if not predictions_path.exists():
                print(f"Warning: Predictions not found for {approach_name}: {predictions_path}")
                continue
            
            result = self.evaluate_predictions(predictions_path, approach_name)
            results[approach_name] = result
        
        return results
    
    @staticmethod
    def print_comparison(results: Dict[str, EvaluationResult]):
        """Print comparison table of all approaches."""
        if not results:
            print("No results to compare")
            return
        
        print("\n" + "=" * 80)
        print("EVALUATION RESULTS COMPARISON")
        print("=" * 80)
        
        # Header
        header = f"{'Approach':<20} {'Syntax':<10} {'Exec':<10} {'Exact':<10} {'Similarity':<12}"
        print(header)
        print("-" * 80)
        
        # Results
        for name, result in sorted(results.items()):
            row = (
                f"{name:<20} "
                f"{result.syntax_rate:>8.1%}  "
                f"{result.execution_rate:>8.1%}  "
                f"{result.exact_match_rate:>8.1%}  "
                f"{result.avg_similarity:>10.4f}"
            )
            print(row)
        
        print("=" * 80)
        
        # Summary
        print("\nLegend:")
        print("  Syntax: Percentage of syntactically correct test cases")
        print("  Exec: Percentage of successfully executable test cases")
        print("  Exact: Percentage of tests that exactly match target path")
        print("  Similarity: Average path similarity score (0-1, LCS-based)")


def parse_args():
    """Parse command line arguments."""
    parser = ArgumentParser(description="Evaluate path coverage benchmark results")
    parser.add_argument("--approaches", type=str, nargs="+",
                        help="Approach names to evaluate (searches results/)")
    parser.add_argument("--files", type=Path, nargs="+",
                        help="Explicit prediction files to evaluate")
    parser.add_argument("--output", type=Path, default=None,
                        help="Output JSON file for results")
    parser.add_argument("--list", action="store_true",
                        help="List available result files")
    return parser.parse_args()


def main():
    """Main entry point."""
    args = parse_args()
    
    # List available results
    if args.list:
        print("Available result files in", RESULTS_DIR)
        for f in sorted(RESULTS_DIR.glob("*.jsonl")):
            print(f"  - {f.name}")
        return
    
    # Build approaches dict
    approaches = {}
    
    if args.files:
        for f in args.files:
            name = f.stem
            approaches[name] = f
    elif args.approaches:
        for name in args.approaches:
            # Search for matching files
            matches = list(RESULTS_DIR.glob(f"*{name}*.jsonl"))
            if matches:
                approaches[name] = matches[0]
            else:
                print(f"Warning: No results found for approach '{name}'")
    else:
        # Auto-discover all result files
        for f in RESULTS_DIR.glob("*.jsonl"):
            name = f.stem.replace("pathcov_", "").split("_gpt")[0]
            approaches[name] = f
    
    if not approaches:
        print("No approaches to evaluate. Use --approaches or --files")
        print("Or run --list to see available results")
        return
    
    print(f"Evaluating {len(approaches)} approaches:")
    for name, path in approaches.items():
        print(f"  - {name}: {path}")
    
    # Run evaluation
    evaluator = Evaluator()
    results = evaluator.evaluate_all(approaches)
    
    # Print comparison
    Evaluator.print_comparison(results)
    
    # Save results if requested
    if args.output:
        output_data = {name: r.to_dict() for name, r in results.items()}
        with open(args.output, 'w') as f:
            json.dump(output_data, f, indent=2)
        print(f"\nResults saved to {args.output}")


if __name__ == "__main__":
    main()
